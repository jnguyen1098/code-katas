{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [Advent of Code 2022 Day 13](https://adventofcode.com/2022/day/13)\n",
    "We love a total ordering!"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import ipytest\n",
    "import pytest\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ansi import *\n",
    "from comp import *\n",
    "ipytest.autoconfig()\n",
    "PART_ONE_SENTINEL = 0x3f3f3f3f + 1\n",
    "PART_TWO_SENTINEL = 0x3f3f3f3f + 2\n",
    "run_doctest_for = lambda func: doctest.run_docstring_examples(func, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Cases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PART_ONE_CASES: dict[str, dict[str, str | int]] = {\n",
    "    \"example\": {\n",
    "        \"example1\": 13,\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"input1\": 5684,\n",
    "    },\n",
    "}\n",
    "PART_ONE_INPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_ONE_CASES.keys()\n",
    "}\n",
    "PART_ONE_OUTPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_ONE_CASES.keys()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PART_TWO_CASES: dict[str, dict[str, str | int]] = {\n",
    "    \"example\": {\n",
    "        \"example1\": 140,\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"input1\": 22932,\n",
    "    },\n",
    "}\n",
    "PART_TWO_INPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_TWO_CASES.keys()\n",
    "}\n",
    "PART_TWO_OUTPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_TWO_CASES.keys()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input Parsing\n",
    "I'm a dirty little `json` abuser yes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model(BaseModel):\n",
    "    data: Any\n",
    "\n",
    "def parse_input_from_filename(filename: str) -> Context:\n",
    "    lines = list(yield_line(filename))\n",
    "\n",
    "    ctx = Context()\n",
    "    ctx.input = []\n",
    "\n",
    "    input_lines = ctx.input\n",
    "\n",
    "    lmao = []\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line == \"\":\n",
    "            input_lines.append(lmao[:])\n",
    "            lmao.clear()\n",
    "            continue\n",
    "        lmao.append(json.loads(line))\n",
    "\n",
    "    input_lines.append(lmao)\n",
    "\n",
    "    return ctx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Parsing Examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name\", PART_ONE_CASES[\"example\"].keys() | PART_TWO_CASES[\"example\"].keys())\n",
    "def test_parsing_examples(test_file_name):\n",
    "    for entity in parse_input_from_filename(test_file_name).input:\n",
    "        enable_logging()\n",
    "        log(f\"{entity}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Parsing Inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name\", PART_ONE_CASES[\"input\"].keys() | PART_TWO_CASES[\"input\"].keys())\n",
    "def test_parsing_inputs(test_file_name):\n",
    "    for entity in parse_input_from_filename(test_file_name).input:\n",
    "        enable_logging()\n",
    "        log(f\"{entity}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integer Comparator\n",
    "3-way compare between two integers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "\n",
    "def compare_ints(num1: int, num2: int) -> int:\n",
    "    if num1 > num2:\n",
    "        return 1\n",
    "    if num1 < num2:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def test_helper_1() -> None:\n",
    "    assert compare_ints(0, 1) == -1\n",
    "    assert compare_ints(1, 0) == 1\n",
    "    assert compare_ints(1, 1) == 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recursive List Comparator\n",
    "If both arguments are integers, then the invariant base case of comparing two integers takes place. Otherwise, there is exactly 1 list and 1 integer, and we convert that integer to a list. Then, we perform a 3-way compare on the resultant lists.\n",
    "\n",
    "The total ordering being asked of in the question is known as \"lexicographical ordering\" whereby the ordering between $a$ and $b$ is defined by the first index for which they differ. For example, \"AAB\" comes after \"AAAA\" because the first index for which they differ is the third letter, in which \"B\" > \"A\". Therefore, \"AAB\" > \"AAAA\". This is different from _shortlex order_ which is used in combinatorics where shorter items always come first (in this case, \"AAB\" < \"AAAA\")\n",
    "\n",
    "Using the same 3-way comparator convention used in C, we return the comparison of $a$ and $b$ as follows:\n",
    "- $-1$ if $a \\lt b$\n",
    "- $0$ if $a = b$\n",
    "- $1$ if $a \\gt b$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "\n",
    "def compare_lists(first: list[Any] | int, second: list[Any] | int) -> int:\n",
    "\n",
    "    t1 = type(first)\n",
    "    t2 = type(second)\n",
    "\n",
    "    if t1 == int and t2 == int:\n",
    "        return compare_ints(first, second)\n",
    "\n",
    "    if t1 == int:\n",
    "        first = [first]\n",
    "\n",
    "    if t2 == int:\n",
    "        second = [second]\n",
    "\n",
    "    for i in range(max(len(first), len(second))):\n",
    "        if i >= len(first):\n",
    "            return -1\n",
    "        if i >= len(second):\n",
    "            return 1\n",
    "        if (val := compare_lists(first[i], second[i])) != 0:\n",
    "            return val\n",
    "\n",
    "    return 0\n",
    "\n",
    "def test_helper_2() -> None:\n",
    "    assert compare_lists([7, 7, 7, 7], [7, 7, 7]) == 1\n",
    "    assert compare_lists([2, 3, 4], [4]) == -1\n",
    "    assert compare_lists([1,1,3,1,1], [1,1,5,1,1]) == -1\n",
    "    assert compare_lists([1,[2,[3,[4,[5,6,7]]]],8,9], [1,[2,[3,[4,[5,6,0]]]],8,9]) == 1\n",
    "    assert compare_lists([[1],[2,3,4]], [[1],4]) == -1\n",
    "    assert compare_lists([[[]]], [[]]) == 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Function\n",
    "Part 1: accumulate all pairs $a$ and $b$'s indices for which $a \\lt b$ in the lexicographic ordering we defined assuming a 1-indexed list.\n",
    "Part 2: sort the entire list using `cmp_to_key` and the 3-way comparator, then return the product of the two sentinel's indices, also 1-indexed. Fortunately the format I used to represent the comparator's outputs is is exactly the format Python (and many other languages) takes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve(part: int, filename: str) -> int:\n",
    "    input = parse_input_from_filename(filename).input\n",
    "    if part == 1:\n",
    "        ans = 0\n",
    "        for idx, pair in enumerate(input):\n",
    "            if compare_lists(pair[0], pair[1]) == -1:\n",
    "                ans += idx + 1\n",
    "        return ans\n",
    "    if part == 2:\n",
    "        all_lines = list(itertools.chain.from_iterable(input))\n",
    "        all_lines.append([[2]])\n",
    "        all_lines.append([[6]])\n",
    "        all_lines.sort(key=cmp_to_key(lambda x, y: compare_lists(x, y)))\n",
    "        return (1 + all_lines.index([[2]])) * (1 + all_lines.index([[6]]))\n",
    "    else:\n",
    "        raise Exception(f\"Invalid part: {part}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_ONE_CASES[\"example\"].items())\n",
    "def test_part_one_examples(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(1, test_file_name)\n",
    "    PART_ONE_OUTPUTS[\"example\"][test_file_name] = test_actual_output\n",
    "    failure_message = \"Did you forget to calibrate the example test case?\" if (\n",
    "        test_expected_output == PART_ONE_SENTINEL\n",
    "    ) else f\"Failed example test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message\n",
    "\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_ONE_CASES[\"input\"].items())\n",
    "def test_part_one_inputs(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(1, test_file_name)\n",
    "    PART_ONE_OUTPUTS[\"input\"][test_file_name] = test_actual_output\n",
    "    failure_message = f\"Candidate answer {test_actual_output} found\" if (\n",
    "        test_expected_output == PART_ONE_SENTINEL\n",
    "    ) else f\"Failed input test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_TWO_CASES[\"example\"].items())\n",
    "def test_part_two_examples(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(2, test_file_name)\n",
    "    PART_TWO_OUTPUTS[\"example\"][test_file_name] = test_actual_output\n",
    "    failure_message = \"Did you forget to calibrate the example test case?\" if (\n",
    "        test_expected_output == PART_TWO_SENTINEL\n",
    "    ) else f\"Failed example test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message\n",
    "\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_TWO_CASES[\"input\"].items())\n",
    "def test_part_two_inputs(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(2, test_file_name)\n",
    "    PART_TWO_OUTPUTS[\"input\"][test_file_name] = test_actual_output\n",
    "    failure_message = f\"Candidate answer {test_actual_output} found\" if (\n",
    "        test_expected_output == PART_TWO_SENTINEL\n",
    "    ) else f\"Failed input test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definitely could've done this one a little faster."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
