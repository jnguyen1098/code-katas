{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [Advent of Code 2022 Day 11](https://adventofcode.com/2022/day/11)\n",
    "\n",
    "I actually did do this before in the AOC 2020 (the bus routes question), so I came pretty prepared.\n",
    "\n",
    "This is the second time CRT has come up. I'm going to steal it off the Internet and put it in my dirty AOC library.\n",
    "\n",
    "Also TIL Python `f`-strings are eagerly evaluated. I had to go on StackOverflow to find a dirty `eval` trick to force deferred evaluation..."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ansi import *\n",
    "from comp import *\n",
    "ipytest.autoconfig()\n",
    "PART_ONE_SENTINEL = 0x3f3f3f3f + 1\n",
    "PART_TWO_SENTINEL = 0x3f3f3f3f + 2\n",
    "run_doctest_for = lambda func: doctest.run_docstring_examples(func, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Cases\n",
    "The gist of this question is optimization. Part 1 and part 2 are the same, just with a higher efficiency ask."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PART_ONE_CASES: dict[str, dict[str, str | int]] = {\n",
    "    \"example\": {\n",
    "        \"example1\": 10605,\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"input1\": 112221,\n",
    "    },\n",
    "}\n",
    "PART_ONE_INPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_ONE_CASES.keys()\n",
    "}\n",
    "PART_ONE_OUTPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_ONE_CASES.keys()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "PART_TWO_CASES: dict[str, dict[str, str | int]] = {\n",
    "    \"example\": {\n",
    "        \"example1\": 2713310158,\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"input1\": 25272176808,\n",
    "    },\n",
    "}\n",
    "PART_TWO_INPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_TWO_CASES.keys()\n",
    "}\n",
    "PART_TWO_OUTPUTS: dict[str, dict[str, str | int]] = {\n",
    "    key: {} for key in PART_TWO_CASES.keys()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input Parsing\n",
    "Using a Pydantic model for Monkey validation helped."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Monkey(BaseModel):\n",
    "    name: str\n",
    "    queue: Deque = Field(default_factory=deque)\n",
    "    actions: int = 0\n",
    "    operation: list[str]\n",
    "    test: int\n",
    "    true_action: int\n",
    "    false_action: int\n",
    "\n",
    "def parse_input_from_filename(filename: str) -> Context:\n",
    "    lines = list(yield_line(filename))\n",
    "\n",
    "    ctx = Context()\n",
    "    ctx.input = []\n",
    "\n",
    "    input_lines = ctx.input\n",
    "    monkey = {}\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Monkey\"):\n",
    "            monkey[\"name\"] = parse(r\"Monkey (\\d+):\", line)[0]\n",
    "        elif line.startswith(\"Starting items\"):\n",
    "            monkey[\"queue\"] = intsep(parse(r\"Starting items: (.*)\", line)[0], \",\")\n",
    "        elif line.startswith(\"Operation:\"):\n",
    "            monkey[\"operation\"] = strsep(parse(r\"Operation: new = (.*)\", line)[0])\n",
    "        elif line.startswith(\"Test: divisible by \"):\n",
    "            monkey[\"test\"] = int(parse(r\"Test: divisible by (\\d+)\", line)[0])\n",
    "        elif line.startswith(\"If true: throw to monkey\"):\n",
    "            monkey[\"true_action\"] = int(parse(r\"If true: throw to monkey (\\d+)\", line)[0])\n",
    "        elif line.startswith(\"If false: throw\"):\n",
    "            monkey[\"false_action\"] = int(parse(r\"If false: throw to monkey (\\d+)\", line)[0])\n",
    "        elif line == \"\":  # Trim off\n",
    "            input_lines.append(Monkey(**monkey))\n",
    "            monkey = {}\n",
    "        else:\n",
    "            raise Exception(f\"ayo {line}\")\n",
    "\n",
    "    input_lines.append(Monkey(**monkey))\n",
    "\n",
    "    return ctx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Parsing Examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m                                                                                            [100%]\u001B[0m\n",
      "============================================= PASSES ==============================================\n",
      "\u001B[32m\u001B[1m_________________________________ test_parsing_examples[example1] _________________________________\u001B[0m\n",
      "-------------------------------------- Captured stderr call ---------------------------------------\n",
      "name='0' queue=deque([79, 98]) actions=0 operation=['old', '*', '19'] test=23 true_action=2 false_action=3\n",
      "name='1' queue=deque([54, 65, 75, 74]) actions=0 operation=['old', '+', '6'] test=19 true_action=2 false_action=0\n",
      "name='2' queue=deque([79, 60, 97]) actions=0 operation=['old', '*', 'old'] test=13 true_action=1 false_action=3\n",
      "name='3' queue=deque([74]) actions=0 operation=['old', '+', '3'] test=17 true_action=0 false_action=1\n",
      "\u001B[32m\u001B[32m\u001B[1m1 passed\u001B[0m\u001B[32m in 0.02s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name\", PART_ONE_CASES[\"example\"].keys() | PART_TWO_CASES[\"example\"].keys())\n",
    "def test_parsing_examples(test_file_name):\n",
    "    for entity in parse_input_from_filename(test_file_name).input:\n",
    "        log(f\"{entity}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Parsing Inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m                                                                                            [100%]\u001B[0m\n",
      "============================================= PASSES ==============================================\n",
      "\u001B[32m\u001B[1m___________________________________ test_parsing_inputs[input1] ___________________________________\u001B[0m\n",
      "-------------------------------------- Captured stderr call ---------------------------------------\n",
      "name='0' queue=deque([54, 98, 50, 94, 69, 62, 53, 85]) actions=0 operation=['old', '*', '13'] test=3 true_action=2 false_action=1\n",
      "name='1' queue=deque([71, 55, 82]) actions=0 operation=['old', '+', '2'] test=13 true_action=7 false_action=2\n",
      "name='2' queue=deque([77, 73, 86, 72, 87]) actions=0 operation=['old', '+', '8'] test=19 true_action=4 false_action=7\n",
      "name='3' queue=deque([97, 91]) actions=0 operation=['old', '+', '1'] test=17 true_action=6 false_action=5\n",
      "name='4' queue=deque([78, 97, 51, 85, 66, 63, 62]) actions=0 operation=['old', '*', '17'] test=5 true_action=6 false_action=3\n",
      "name='5' queue=deque([88]) actions=0 operation=['old', '+', '3'] test=7 true_action=1 false_action=0\n",
      "name='6' queue=deque([87, 57, 63, 86, 87, 53]) actions=0 operation=['old', '*', 'old'] test=11 true_action=5 false_action=0\n",
      "name='7' queue=deque([73, 59, 82, 65]) actions=0 operation=['old', '+', '6'] test=2 true_action=4 false_action=3\n",
      "\u001B[32m\u001B[32m\u001B[1m1 passed\u001B[0m\u001B[32m in 0.02s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name\", PART_ONE_CASES[\"input\"].keys() | PART_TWO_CASES[\"input\"].keys())\n",
    "def test_parsing_inputs(test_file_name):\n",
    "    for entity in parse_input_from_filename(test_file_name).input:\n",
    "        log(f\"{entity}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get Math Operation\n",
    "Should've used `eval`..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                                                                         [100%]\u001B[0m\n",
      "============================================= PASSES ==============================================\n",
      "\u001B[32m\u001B[32m\u001B[1m4 passed\u001B[0m\u001B[32m in 0.02s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "def get_math_op(op: str) -> Callable[[int, int], int]:\n",
    "    \"\"\"\n",
    "    Takes a symbol representing a mathematical operation and returns an apply function implementing it.\n",
    "\n",
    "    :param op: the operation string from the input\n",
    "    :return: the apply function that implements this\n",
    "    \"\"\"\n",
    "    if op == \"+\":\n",
    "        return lambda x, y: x + y\n",
    "    if op == \"*\":\n",
    "        return lambda x, y: x * y\n",
    "    if op == \"/\":\n",
    "        return lambda x, y: x // y\n",
    "    if op == \"-\":\n",
    "        return lambda x, y: x - y\n",
    "    raise NotImplementedError(f\"Unimplemented operator string: {op}\")\n",
    "\n",
    "@pytest.mark.parametrize(\"arg, case\", [(\"+\", (1, 2, 3)), (\"*\", (3, 4, 12)), (\"/\", (12, 3, 4)), (\"-\", (34, 10, 24))])\n",
    "def test_get_math_op(arg: str, case: tuple[int, int, int]) -> None:\n",
    "    operation = get_math_op(arg)\n",
    "    lvalue, rvalue, expected = case\n",
    "    assert operation(lvalue, rvalue) == expected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get Monkey Operation\n",
    "Really should've used `eval`..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m                                                                                            [100%]\u001B[0m\n",
      "============================================= PASSES ==============================================\n",
      "\u001B[32m\u001B[32m\u001B[1m1 passed\u001B[0m\u001B[32m in 0.01s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "def get_monkey_op(args: list[str]):\n",
    "    \"\"\"\n",
    "    Takes the monkey's operation tokens and returns the relevant apply function.\n",
    "\n",
    "    :param args: LVALUE OP RVALUE\n",
    "    :return: apply function\n",
    "    \"\"\"\n",
    "    lvalue, op, rvalue = args\n",
    "    apply = get_math_op(op)\n",
    "    if lvalue == \"old\" and rvalue == \"old\":\n",
    "        return lambda old: apply(old, old)\n",
    "    elif lvalue == \"old\" and rvalue != \"old\":\n",
    "        return lambda old: apply(old, int(rvalue))\n",
    "    elif lvalue != \"old\" and rvalue == \"old\":\n",
    "        return lambda old: apply(int(lvalue), old)\n",
    "    elif lvalue != \"old\" and rvalue != \"old\":\n",
    "        return lambda old: apply(int(lvalue), int(rvalue))\n",
    "    raise Exception(f\"Bad configuration of {args=}\")\n",
    "\n",
    "def test_get_monkey_op() -> None:\n",
    "    assert get_monkey_op([\"old\", \"*\", \"old\"])(10) == 100\n",
    "    assert get_monkey_op([\"old\", \"+\", \"old\"])(10) == 20\n",
    "    assert get_monkey_op([\"old\", \"+\", \"20\"])(10) == 30\n",
    "    assert get_monkey_op([\"50\", \"-\", \"20\"])(1234554320) == 30"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monkey Processor\n",
    "This will simulate the monkey's processing. It isn't idempotent as there's a side effect of incrementing the monkey's `action` counter.\n",
    "In theory I should be able to just take the length of its queue and use that as the incrementer..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def process_monkey(monkey: Monkey, all_monkeys: list[Monkey], do_crt: bool = False) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Processes a monkey's queue, updating its action count and returning a list of its updated worry value requests.\n",
    "\n",
    "    :param monkey: the monkey whose queue needs to be processed\n",
    "    :param all_monkeys: list of all monkeys (used to extract pairwise moduli)\n",
    "    :param do_crt: flag to determine whether to reduce all worry values using Chinese Remainder Theorem\n",
    "    :return: a list of the monkey's requests after processing the operations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    moduli = tuple([monk.test for monk in all_monkeys])\n",
    "    while monkey.queue:\n",
    "        monkey.actions += 1\n",
    "        popped_worry_value = monkey.queue.popleft()\n",
    "        monkey_operation = get_monkey_op(monkey.operation)\n",
    "        modified_worry_value = monkey_operation(popped_worry_value)\n",
    "        final_worry_value = modified_worry_value // (1 if do_crt else 3)\n",
    "        satisfied = final_worry_value % monkey.test == 0\n",
    "        next_monkey = monkey.false_action if not satisfied else monkey.true_action\n",
    "        if do_crt:\n",
    "            residues = tuple([final_worry_value] * len(all_monkeys))\n",
    "            final_worry_value = solve_chinese_remainder_theorem(residues, moduli)\n",
    "        else:\n",
    "            log(defer(f\"Monkey inspects an item with a worry level of {popped_worry_value}\"))\n",
    "            log(defer(f\"Worry level goes from {popped_worry_value} to {modified_worry_value}\"))\n",
    "            log(defer(f\"Monkey gets bored with item. Worry level is divided by 3 to {final_worry_value}\"))\n",
    "            log(defer(f\"Current worry level {'is' if satisfied else 'is not'} divisible by {monkey.test}\"))\n",
    "            log(defer(f\"Item with worry level {final_worry_value} is thrown to monkey {next_monkey}\"))\n",
    "            log(defer(\"\"))\n",
    "        results.append((final_worry_value, next_monkey))\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Score Getter\n",
    "Every time a monkey is processed, the function returns a list containing the monkey's newly-declared worry values, along with the next monkey to receive them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_score_after_round(round_number: int, monkeys: list[Monkey], do_crt: bool = False) -> int:\n",
    "    \"\"\"\n",
    "    Conducts `round_number` rounds (where each found, all monkeys have their queues processed).\n",
    "    Then, returns the product of the two highest monkey action counts.\n",
    "\n",
    "    :param round_number: how many rounds to conduct\n",
    "    :param monkeys: the list of monkeys that are being processed\n",
    "    :param do_crt: whether to perform Chinese Remainder Theorem\n",
    "    :return: the product of the two most active monkeys' action counts\n",
    "    \"\"\"\n",
    "    for _ in range(round_number):\n",
    "        for monkey in monkeys:\n",
    "            for new_worry_value, receiving_monkey_idx in process_monkey(monkey, monkeys, do_crt=do_crt):\n",
    "                monkeys[receiving_monkey_idx].queue.append(new_worry_value)\n",
    "    return prod(heapq.nlargest(2, [monkey.actions for monkey in monkeys]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Function\n",
    "What's stopping me from just unconditionally doing CRT is that you don't divide by 3 in the unbounded version."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def solve(part: int, filename: str) -> int:\n",
    "    monkeys = parse_input_from_filename(filename).input\n",
    "    if part == 1:\n",
    "        disable_logging()\n",
    "        return get_score_after_round(20, monkeys)\n",
    "    if part == 2:\n",
    "        disable_logging()\n",
    "        return get_score_after_round(10000, monkeys, do_crt=True)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid part: {part}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                                                                           [100%]\u001B[0m\n",
      "============================================= PASSES ==============================================\n",
      "\u001B[32m\u001B[32m\u001B[1m2 passed\u001B[0m\u001B[32m in 0.15s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_ONE_CASES[\"example\"].items())\n",
    "def test_part_one_examples(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(1, test_file_name)\n",
    "    PART_ONE_OUTPUTS[\"example\"][test_file_name] = test_actual_output\n",
    "    failure_message = \"Did you forget to calibrate the example test case?\" if (\n",
    "        test_expected_output == PART_ONE_SENTINEL\n",
    "    ) else f\"Failed example test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message\n",
    "\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_ONE_CASES[\"input\"].items())\n",
    "def test_part_one_inputs(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(1, test_file_name)\n",
    "    PART_ONE_OUTPUTS[\"input\"][test_file_name] = test_actual_output\n",
    "    failure_message = f\"Candidate answer {test_actual_output} found\" if (\n",
    "        test_expected_output == PART_ONE_SENTINEL\n",
    "    ) else f\"Failed input test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                                                                           [100%]\u001B[0m\n",
      "============================================= PASSES ==============================================\n",
      "\u001B[32m\u001B[32m\u001B[1m2 passed\u001B[0m\u001B[32m in 4.04s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -xrPvvvvv\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_TWO_CASES[\"example\"].items())\n",
    "def test_part_two_examples(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(2, test_file_name)\n",
    "    PART_TWO_OUTPUTS[\"example\"][test_file_name] = test_actual_output\n",
    "    failure_message = \"Did you forget to calibrate the example test case?\" if (\n",
    "            test_expected_output == PART_TWO_SENTINEL\n",
    "    ) else f\"Failed example test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message\n",
    "\n",
    "@pytest.mark.parametrize(\"test_file_name, test_expected_output\", PART_TWO_CASES[\"input\"].items())\n",
    "def test_part_two_inputs(test_file_name, test_expected_output):\n",
    "    test_actual_output = solve(2, test_file_name)\n",
    "    PART_TWO_OUTPUTS[\"input\"][test_file_name] = test_actual_output\n",
    "    failure_message = f\"Candidate answer {test_actual_output} found\" if (\n",
    "            test_expected_output == PART_TWO_SENTINEL\n",
    "    ) else f\"Failed input test case: expected {test_expected_output} but got {test_actual_output}\"\n",
    "    assert test_actual_output == test_expected_output, failure_message"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not too bad. Definitely room for optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
